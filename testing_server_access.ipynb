{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for setup\n",
    "\n",
    "- Install Ollama on the host server\n",
    "- Run: `sudo systemctl edit ollama.service`\n",
    "- Append the following under \"Anything between here and the comment below will become the new contents of the file\"\n",
    "[Service]\n",
    "Environment=\"OLLAMA_HOST=0.0.0.0\"\n",
    "- Run: `sudo systemctl daemon-reload && sudo systemctl restart ollama`\n",
    "- Connect via the following ssh command: `ssh -L 8080:localhost:11434 jason-server@some_host_Address -p 8000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests\n",
    "# !pip install openai\n",
    "# !pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "- llama3.3:70b-instruct-q4_0\n",
      "- llama3.2-vision:11b-instruct-fp16\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "#Run this ssh -L 8080:localhost:11434 jason-server@dataarch.ddns.net -p 8000\n",
    "\n",
    "# Use the specific IP address you provided\n",
    "OLLAMA_HOST = \"http://localhost:8080\"\n",
    "\n",
    "# Endpoint to list available models\n",
    "url = f\"{OLLAMA_HOST}/api/tags\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()  # Parse JSON response\n",
    "        print(\"Available models:\")\n",
    "        for model in models.get('models', []):\n",
    "            print(f\"- {model.get('name', 'Unknown model')}\")\n",
    "    else:\n",
    "        print(f\"Failed to access Ollama service. Status code: {response.status_code}\")\n",
    "        print(f\"Response: {response.text}\")\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing connection to: http://localhost:8080/api/tags\n",
      "Connection test status code: 200\n",
      "Attempting to create chat completion...\n",
      "Response received successfully!\n",
      "The Los Angeles Dodgers won the World Series in 2020, defeating the Tampa Bay Rays in six games (4-2). It was their first championship since 1988.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "import traceback\n",
    "\n",
    "try:\n",
    "    # First, verify server connectivity\n",
    "    connection_test_url = f\"{OLLAMA_HOST}/api/tags\"\n",
    "    print(f\"Testing connection to: {connection_test_url}\")\n",
    "    \n",
    "    connection_test = requests.get(connection_test_url, timeout=5)\n",
    "    print(f\"Connection test status code: {connection_test.status_code}\")\n",
    "    \n",
    "    # If connection test passes, proceed with OpenAI client\n",
    "    client = OpenAI(\n",
    "        base_url = f\"{OLLAMA_HOST}/v1\",\n",
    "        api_key='ollama',  # required, but unused\n",
    "    )\n",
    "\n",
    "    print(\"Attempting to create chat completion...\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama3.3:70b-instruct-q4_0\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"Response received successfully!\")\n",
    "    print(response.choices[0].message.content)\n",
    "\n",
    "except requests.exceptions.RequestException as req_err:\n",
    "    print(f\"Request Error: {req_err}\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from openai import OpenAI\n",
    "import traceback\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "def create_ollama_client(ollama_host):\n",
    "    \"\"\"\n",
    "    Creates an Ollama client with text and image processing capabilities.\n",
    "    \n",
    "    Args:\n",
    "        ollama_host (str): The host URL for the Ollama server\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing process_text_query and process_image_query functions\n",
    "        \n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If server connection fails\n",
    "        Exception: For other unexpected errors\n",
    "    \"\"\"\n",
    "    def encode_image_to_base64(image_path):\n",
    "        \"\"\"\n",
    "        Convert an image file to base64 string\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(image_path, 'rb') as image_file:\n",
    "                return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding image: {e}\")\n",
    "            return None\n",
    "\n",
    "    try:\n",
    "        # First, verify server connectivity\n",
    "        connection_test_url = f\"{ollama_host}/api/tags\"\n",
    "        print(f\"Testing connection to: {connection_test_url}\")\n",
    "        \n",
    "        connection_test = requests.get(connection_test_url, timeout=5)\n",
    "        print(f\"Connection test status code: {connection_test.status_code}\")\n",
    "        \n",
    "        # If connection test passes, proceed with OpenAI client\n",
    "        client = OpenAI(\n",
    "            base_url = f\"{ollama_host}/v1\",\n",
    "            api_key='ollama',  # required, but unused\n",
    "        )\n",
    "\n",
    "        def process_text_query(prompt, model=\"llama3.3:70b-instruct-q4_0\"):\n",
    "            \"\"\"\n",
    "            Process a text-based query\n",
    "            \n",
    "            Args:\n",
    "                prompt (str): The text prompt to process\n",
    "                model (str): The model to use for processing\n",
    "                \n",
    "            Returns:\n",
    "                str: The model's response\n",
    "            \"\"\"\n",
    "            print(\"Attempting to create chat completion...\")\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        def process_image_query(image_path, prompt, model=\"llama3.2-vision:11b-instruct-fp16\"):\n",
    "            \"\"\"\n",
    "            Process an image-based query\n",
    "            \n",
    "            Args:\n",
    "                image_path (str): Path to the image file\n",
    "                prompt (str): The text prompt to process along with the image\n",
    "                model (str): The model to use for processing\n",
    "                \n",
    "            Returns:\n",
    "                str: The model's response\n",
    "            \"\"\"\n",
    "            print(\"Processing image query...\")\n",
    "            base64_image = encode_image_to_base64(image_path)\n",
    "            \n",
    "            if not base64_image:\n",
    "                return \"Error: Could not process image\"\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\"type\": \"text\", \"text\": prompt},\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                                }\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "\n",
    "        return {\n",
    "            \"process_text_query\": process_text_query,\n",
    "            \"process_image_query\": process_image_query\n",
    "        }\n",
    "\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"Request Error: {req_err}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example image response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing connection to: http://localhost:8080/api/tags\n",
      "Connection test status code: 200\n",
      "Processing image query...\n",
      "\n",
      "Image Response:\n",
      "There does not appear to be an image attached here. However, I can tell you about different types of tabby cats that may look like this if there was an image attached. Tabby cats come in various patterns and colors, but they all share a distinctive M-shaped marking on the forehead that resembles a tiger's stripes.\n",
      "\n",
      "**There are four known varieties of tabbies:**\n",
      "\n",
      "1. Classic Tabbies\n",
      "2. Spotted Tabbies\n",
      "3. Mackerel (or Ticked) tabbies\n",
      "4. Blotched Tabbies\n",
      "\n",
      "The exact breed or type may be difficult to determine without an image, but if there was one attached we could try to identify it with more certainty.\n"
     ]
    }
   ],
   "source": [
    "# Use the specific IP address you provided\n",
    "OLLAMA_HOST = \"http://localhost:8080\"\n",
    "ollama_client = create_ollama_client(OLLAMA_HOST)\n",
    "\n",
    "# Example image query\n",
    "image_response = ollama_client[\"process_image_query\"](\n",
    "    os.path.join(os.getcwd(), \"tiger.jpg\"),\n",
    "    \"What is the breed of the cat is in the image?\"\n",
    ")\n",
    "print(\"\\nImage Response:\")\n",
    "print(image_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example text response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing connection to: http://localhost:8080/api/tags\n",
      "Connection test status code: 200\n",
      "Attempting to create chat completion...\n",
      "Τα Tabby γάτα είναι μια ποικιλία घरσκής γάτας που φέρει ένα χαρακτηριστικό μοτίβォ στο τρίχωμα τους, το οποίο θυμίζει τη σκούρα και λαμπρόλευκη ξύλινη επιφάνεια ενός παλτού. Αυτό το μοτίβο μπορεί να είναι λευκό με μαύρες ραβδώσεις ή σηмеία, αυξάνοντας την ομορφιά και την εξαίρεση των γατών αυτής της φυλής. Τα Tabby γάτα επίσης έχουν τα χαρκτηριστικά,大的, καστανό-πράσινα μάτια, καθώς και ένα μικρό ρινικό σώμα με μακρύ λαιμό.\n"
     ]
    }
   ],
   "source": [
    "# Use the specific IP address you provided\n",
    "OLLAMA_HOST = \"http://localhost:8080\"\n",
    "ollama_client = create_ollama_client(OLLAMA_HOST)\n",
    "\n",
    "# Example image query\n",
    "text_response = ollama_client[\"process_text_query\"](\n",
    "    \"Write a paragraph about the charecteristics of tabby cats in Greek.\"\n",
    ")\n",
    "print(text_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
